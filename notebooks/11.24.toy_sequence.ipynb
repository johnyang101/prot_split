{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = torch.load('../data/esmvalset2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"../google_prot_fns/\"\n",
    "filters = 64\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "protein_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a toy example we only select the largest 2 protein families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_family_number = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wp9lxVA7_42k"
   },
   "outputs": [],
   "source": [
    "def read_data(name_sub_folder):\n",
    "  full_data = []\n",
    "  for f in os.listdir(os.path.join(dataset_folder, name_sub_folder)):\n",
    "    data = pd.read_csv(os.path.join(dataset_folder, name_sub_folder, f))\n",
    "    full_data.append(data)\n",
    "  return pd.concat(full_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hocFI7D_AoAA",
    "outputId": "4e690d16-5c6f-4881-f655-286b9ca0cd98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_id</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>family_accession</th>\n",
       "      <th>aligned_sequence</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRANC</td>\n",
       "      <td>V234_FOWPN/317-415</td>\n",
       "      <td>PF09372.10</td>\n",
       "      <td>FQDLSLFDLLSNEDN..IAIVYRLSDTLLEKMN...II..KTIFPN...</td>\n",
       "      <td>FQDLSLFDLLSNEDNIAIVYRLSDTLLEKMNIIKTIFPNCFRIIQN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LytR_C</td>\n",
       "      <td>S5V4G6_STRC3/409-501</td>\n",
       "      <td>PF13399.6</td>\n",
       "      <td>RIAV.QVRNG.TGADGQAPVPQR.AGVIAQVL...Q....GKG.FT...</td>\n",
       "      <td>RIAVQVRNGTGADGQAPVPQRAGVIAQVLQGKGFTEAAKDTTAAAE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zn_peptidase_2</td>\n",
       "      <td>R6B6D7_9CLOT/8-223</td>\n",
       "      <td>PF04298.12</td>\n",
       "      <td>.VILMLIIIVLPLYANIKINSTYSKY.SKKQNSG..RLTGKEVAEK...</td>\n",
       "      <td>VILMLIIIVLPLYANIKINSTYSKYSKKQNSGRLTGKEVAEKILEM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UvrC_HhH_N</td>\n",
       "      <td>UVRC_MYCGA/371-525</td>\n",
       "      <td>PF08459.11</td>\n",
       "      <td>KKLIKVD.K.LN..HLEVYDNSN.LFNTDK.VSAMIVFEN..NQFN...</td>\n",
       "      <td>KKLIKVDKLNHLEVYDNSNLFNTDKVSAMIVFENNQFNKKKYRKYK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sedlin_N</td>\n",
       "      <td>Q7QCA5_ANOGA/9-136</td>\n",
       "      <td>PF04628.13</td>\n",
       "      <td>IVGHNDNPIFETEFVTVN..KEAKKE.......D.......HRHL....</td>\n",
       "      <td>IVGHNDNPIFETEFVTVNKEAKKEDHRHLNQFIAHAALDLIDEHKW...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        family_id         sequence_name family_accession  \\\n",
       "0           PRANC    V234_FOWPN/317-415       PF09372.10   \n",
       "1          LytR_C  S5V4G6_STRC3/409-501        PF13399.6   \n",
       "2  Zn_peptidase_2    R6B6D7_9CLOT/8-223       PF04298.12   \n",
       "3      UvrC_HhH_N    UVRC_MYCGA/371-525       PF08459.11   \n",
       "4        Sedlin_N    Q7QCA5_ANOGA/9-136       PF04628.13   \n",
       "\n",
       "                                    aligned_sequence  \\\n",
       "0  FQDLSLFDLLSNEDN..IAIVYRLSDTLLEKMN...II..KTIFPN...   \n",
       "1  RIAV.QVRNG.TGADGQAPVPQR.AGVIAQVL...Q....GKG.FT...   \n",
       "2  .VILMLIIIVLPLYANIKINSTYSKY.SKKQNSG..RLTGKEVAEK...   \n",
       "3  KKLIKVD.K.LN..HLEVYDNSN.LFNTDK.VSAMIVFEN..NQFN...   \n",
       "4  IVGHNDNPIFETEFVTVN..KEAKKE.......D.......HRHL....   \n",
       "\n",
       "                                            sequence  \n",
       "0  FQDLSLFDLLSNEDNIAIVYRLSDTLLEKMNIIKTIFPNCFRIIQN...  \n",
       "1  RIAVQVRNGTGADGQAPVPQRAGVIAQVLQGKGFTEAAKDTTAAAE...  \n",
       "2  VILMLIIIVLPLYANIKINSTYSKYSKKQNSGRLTGKEVAEKILEM...  \n",
       "3  KKLIKVDKLNHLEVYDNSNLFNTDKVSAMIVFENNQFNKKKYRKYK...  \n",
       "4  IVGHNDNPIFETEFVTVNKEAKKEDHRHLNQFIAHAALDLIDEHKW...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = read_data(\"train\")\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oPrHLjsBPKu",
    "outputId": "9c177ce6-1521-4f17-8fcc-3920d76948af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_id</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>family_accession</th>\n",
       "      <th>aligned_sequence</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GNAT_acetyltran</td>\n",
       "      <td>R6RQF6_9CLOT/17-251</td>\n",
       "      <td>PF12746.7</td>\n",
       "      <td>AFLFSGR..REVMAD....ACLQGMM..GCVYG..........TAG...</td>\n",
       "      <td>AFLFSGRREVMADACLQGMMGCVYGTAGGMDSAAAVLGDFCFLAGK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MoaC</td>\n",
       "      <td>W5NKR5_LEPOC/505-640</td>\n",
       "      <td>PF01967.21</td>\n",
       "      <td>MVDVGGK.PVSRRTAAASATVLLG.EK..........AFWLV.......</td>\n",
       "      <td>MVDVGGKPVSRRTAAASATVLLGEKAFWLVKENQLAKGDALAVAQI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Methyltransf_25</td>\n",
       "      <td>C0QLU8_DESAH/50-147</td>\n",
       "      <td>PF13649.6</td>\n",
       "      <td>VLDVACGT.C...D..VA...ME..AR.NQ.......T....G......</td>\n",
       "      <td>VLDVACGTCDVAMEARNQTGDAAFIIGTDFSPGMLTLGLQKLKKNR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMG1</td>\n",
       "      <td>T1G7Q2_HELRO/22-222</td>\n",
       "      <td>PF03587.14</td>\n",
       "      <td>VVLERASLESVKV..G.................KEYQLLN....CD...</td>\n",
       "      <td>VVLERASLESVKVGKEYQLLNCDRHKGIAKKFKRDISTCRPDITHQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Glyco_hydro_30C</td>\n",
       "      <td>C6VRM9_DYAFD/453-540</td>\n",
       "      <td>PF17189.4</td>\n",
       "      <td>GAVRVDVSGGLGTD...............AMVVSSYLN..TDKSLV...</td>\n",
       "      <td>GAVRVDVSGGLGTDAMVVSSYLNTDKSLVTVIVNADNQDRDISLAI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         family_id         sequence_name family_accession  \\\n",
       "0  GNAT_acetyltran   R6RQF6_9CLOT/17-251        PF12746.7   \n",
       "1             MoaC  W5NKR5_LEPOC/505-640       PF01967.21   \n",
       "2  Methyltransf_25   C0QLU8_DESAH/50-147        PF13649.6   \n",
       "3             EMG1   T1G7Q2_HELRO/22-222       PF03587.14   \n",
       "4  Glyco_hydro_30C  C6VRM9_DYAFD/453-540        PF17189.4   \n",
       "\n",
       "                                    aligned_sequence  \\\n",
       "0  AFLFSGR..REVMAD....ACLQGMM..GCVYG..........TAG...   \n",
       "1  MVDVGGK.PVSRRTAAASATVLLG.EK..........AFWLV.......   \n",
       "2  VLDVACGT.C...D..VA...ME..AR.NQ.......T....G......   \n",
       "3  VVLERASLESVKV..G.................KEYQLLN....CD...   \n",
       "4  GAVRVDVSGGLGTD...............AMVVSSYLN..TDKSLV...   \n",
       "\n",
       "                                            sequence  \n",
       "0  AFLFSGRREVMADACLQGMMGCVYGTAGGMDSAAAVLGDFCFLAGK...  \n",
       "1  MVDVGGKPVSRRTAAASATVLLGEKAFWLVKENQLAKGDALAVAQI...  \n",
       "2  VLDVACGTCDVAMEARNQTGDAAFIIGTDFSPGMLTLGLQKLKKNR...  \n",
       "3  VVLERASLESVKVGKEYQLLNCDRHKGIAKKFKRDISTCRPDITHQ...  \n",
       "4  GAVRVDVSGGLGTDAMVVSSYLNTDKSLVTVIVNADNQDRDISLAI...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = read_data(\"test\")\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8VCt7qRBTbF",
    "outputId": "1c007d65-c080-40fa-9c07-5796e190806c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_id</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>family_accession</th>\n",
       "      <th>aligned_sequence</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zf-Tim10_DDP</td>\n",
       "      <td>N1QB11_PSEFD/15-76</td>\n",
       "      <td>PF02953.15</td>\n",
       "      <td>..RMEKKQMKDFMNMYSNLVQRCFNDCV...........TD.F......</td>\n",
       "      <td>RMEKKQMKDFMNMYSNLVQRCFNDCVTDFTSKSLQSKEEGCVMRCV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DNA_primase_S</td>\n",
       "      <td>A8XA78_CAEBR/105-345</td>\n",
       "      <td>PF01896.19</td>\n",
       "      <td>FDID..LTDYDNIRNCCKEATVCPKCWKFMVLAVKILDFLLDDMFG...</td>\n",
       "      <td>FDIDLTDYDNIRNCCKEATVCPKCWKFMVLAVKILDFLLDDMFGFN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Col_cuticle_N</td>\n",
       "      <td>A8XBM5_CAEBR/9-56</td>\n",
       "      <td>PF01484.17</td>\n",
       "      <td>ASAAILSGATIVGCLFFAAQIFNEVNSLYDDVMVDMDAFKVKSNIA...</td>\n",
       "      <td>ASAAILSGATIVGCLFFAAQIFNEVNSLYDDVMVDMDAFKVKSNIAWD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GST_C_3</td>\n",
       "      <td>W4XBU3_STRPU/120-207</td>\n",
       "      <td>PF14497.6</td>\n",
       "      <td>KD.................................KLKESLPKTVN...</td>\n",
       "      <td>KDKLKESLPKTVNPILLKFLEKALEDNPNGNGYFVGQDATMVEFVY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ada_Zn_binding</td>\n",
       "      <td>E8U5K2_DEIML/9-73</td>\n",
       "      <td>PF02805.16</td>\n",
       "      <td>DRWQAVVQRE...AAQ.DG...LFLYAVRTTGIYCRPSCPSRRPR....</td>\n",
       "      <td>DRWQAVVQREAAQDGLFLYAVRTTGIYCRPSCPSRRPRRENVTFFE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        family_id         sequence_name family_accession  \\\n",
       "0    zf-Tim10_DDP    N1QB11_PSEFD/15-76       PF02953.15   \n",
       "1   DNA_primase_S  A8XA78_CAEBR/105-345       PF01896.19   \n",
       "2   Col_cuticle_N     A8XBM5_CAEBR/9-56       PF01484.17   \n",
       "3         GST_C_3  W4XBU3_STRPU/120-207        PF14497.6   \n",
       "4  Ada_Zn_binding     E8U5K2_DEIML/9-73       PF02805.16   \n",
       "\n",
       "                                    aligned_sequence  \\\n",
       "0  ..RMEKKQMKDFMNMYSNLVQRCFNDCV...........TD.F......   \n",
       "1  FDID..LTDYDNIRNCCKEATVCPKCWKFMVLAVKILDFLLDDMFG...   \n",
       "2  ASAAILSGATIVGCLFFAAQIFNEVNSLYDDVMVDMDAFKVKSNIA...   \n",
       "3  KD.................................KLKESLPKTVN...   \n",
       "4  DRWQAVVQRE...AAQ.DG...LFLYAVRTTGIYCRPSCPSRRPR....   \n",
       "\n",
       "                                            sequence  \n",
       "0  RMEKKQMKDFMNMYSNLVQRCFNDCVTDFTSKSLQSKEEGCVMRCV...  \n",
       "1  FDIDLTDYDNIRNCCKEATVCPKCWKFMVLAVKILDFLLDDMFGFN...  \n",
       "2   ASAAILSGATIVGCLFFAAQIFNEVNSLYDDVMVDMDAFKVKSNIAWD  \n",
       "3  KDKLKESLPKTVNPILLKFLEKALEDNPNGNGYFVGQDATMVEFVY...  \n",
       "4  DRWQAVVQREAAQDGLFLYAVRTTGIYCRPSCPSRRPRRENVTFFE...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev = read_data(\"dev\")\n",
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PF13649.6     3637\n",
       "PF00560.33    1927\n",
       "PF13508.7     1761\n",
       "PF06580.13    1537\n",
       "PF02397.16    1528\n",
       "              ... \n",
       "PF17595.2        1\n",
       "PF15895.5        1\n",
       "PF09125.10       1\n",
       "PF07407.11       1\n",
       "PF14586.6        1\n",
       "Name: family_accession, Length: 17929, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['family_accession'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = pd.concat([dataset_train, dataset_test, dataset_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the data from Google format to a Fasta file.\n",
    "ofile = open(\"all_datasets_fasta.fasta\", \"w\")\n",
    "\n",
    "for row in all_datasets.iterrows():\n",
    "\n",
    "    ofile.write(\">\" + row[1]['sequence_name'] + \"\\n\" + row[1]['sequence'] + \"\\n\")\n",
    "\n",
    "    #do not forget to close it\n",
    "\n",
    "ofile.close()all_datasetsall_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = all_datasets['family_accession'].value_counts()[:protein_family_number].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMRK7Vn2jC1W"
   },
   "outputs": [],
   "source": [
    "# def return_from_dataset(dataset, classes):\n",
    "#   return [dataset.loc[dataset['family_accession'].isin(classes)].reset_index(), classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoGgkSF2jTRY"
   },
   "outputs": [],
   "source": [
    "# [toy_dataset, classes] = return_from_dataset(all_datasets, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_dataset.to_csv(\"../data/2_class.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset to save memory. Above is an example for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"../google_prot_fns/\"\n",
    "filters = 64\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "protein_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    }
   ],
   "source": [
    "esm_model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
    "# model_data = torch.load('/data/rsg/chemistry/johnyang/home/pt_models/esm2_t33_650M_UR50D.pt')\n",
    "# regression_data = torch.load('/data/rsg/chemistry/johnyang/home/pt_models/esm2_t33_650M_UR50D-contact-regression.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from typing import Sequence, Tuple, List, Union\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "RawMSA = Sequence[Tuple[str, str]]\n",
    "class FastaBatchedDataset(object):\n",
    "    def __init__(self, sequence_labels, sequence_strs):\n",
    "        self.sequence_labels = list(sequence_labels)\n",
    "        self.sequence_strs = list(sequence_strs)\n",
    "    @classmethod\n",
    "    def from_file(cls, fasta_file):\n",
    "        sequence_labels, sequence_strs = [], []\n",
    "        cur_seq_label = None\n",
    "        buf = []\n",
    "        def _flush_current_seq():\n",
    "            nonlocal cur_seq_label, buf\n",
    "            if cur_seq_label is None:\n",
    "                return\n",
    "            sequence_labels.append(cur_seq_label)\n",
    "            sequence_strs.append(\"\".join(buf))\n",
    "            cur_seq_label = None\n",
    "            buf = []\n",
    "        with open(fasta_file, \"r\") as infile:\n",
    "            for line_idx, line in enumerate(infile):\n",
    "                if line.startswith(\">\"):  # label line\n",
    "                    _flush_current_seq()\n",
    "                    line = line[1:].strip()\n",
    "                    if len(line) > 0:\n",
    "                        cur_seq_label = line\n",
    "                    else:\n",
    "                        cur_seq_label = f\"seqnum{line_idx:09d}\"\n",
    "                else:  # sequence line\n",
    "                    buf.append(line.strip())\n",
    "        _flush_current_seq()\n",
    "        assert len(set(sequence_labels)) == len(\n",
    "            sequence_labels\n",
    "        ), \"Found duplicate sequence labels\"\n",
    "        return cls(sequence_labels, sequence_strs)\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequence_labels[idx], self.sequence_strs[idx]\n",
    "    def get_batch_indices(self, toks_per_batch, extra_toks_per_seq=0):\n",
    "        sizes = [(len(s), i) for i, s in enumerate(self.sequence_strs)]\n",
    "        sizes.sort()\n",
    "        batches = []\n",
    "        buf = []\n",
    "        max_len = 0\n",
    "        def _flush_current_buf():\n",
    "            nonlocal max_len, buf\n",
    "            if len(buf) == 0:\n",
    "                return\n",
    "            batches.append(buf)\n",
    "            buf = []\n",
    "            max_len = 0\n",
    "        for sz, i in sizes:\n",
    "            sz += extra_toks_per_seq\n",
    "            if max(sz, max_len) * (len(buf) + 1) > toks_per_batch:\n",
    "                _flush_current_buf()\n",
    "            max_len = max(max_len, sz)\n",
    "            buf.append(i)\n",
    "        _flush_current_buf()\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# esm_model, alphabet = load_model_and_alphabet_core(\"esm2_t33_650M_UR50D\", model_data, regression_data)\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "esm_model.to('cuda')\n",
    "esm_model.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n",
      "Read ./all_datasets_fasta.fasta with 1339083 sequences\n",
      "Processing 1 of 2096 batches (4761 sequences)\n",
      "Processing 2 of 2096 batches (4166 sequences)\n",
      "Processing 3 of 2096 batches (4000 sequences)\n",
      "Processing 4 of 2096 batches (3703 sequences)\n",
      "Processing 5 of 2096 batches (3571 sequences)\n",
      "Processing 6 of 2096 batches (3448 sequences)\n",
      "Processing 7 of 2096 batches (3333 sequences)\n",
      "Processing 8 of 2096 batches (3225 sequences)\n",
      "Processing 9 of 2096 batches (3030 sequences)\n",
      "Processing 10 of 2096 batches (2941 sequences)\n",
      "Processing 11 of 2096 batches (2857 sequences)\n",
      "Processing 12 of 2096 batches (2857 sequences)\n",
      "Processing 13 of 2096 batches (2777 sequences)\n",
      "Processing 14 of 2096 batches (2777 sequences)\n",
      "Processing 15 of 2096 batches (2702 sequences)\n",
      "Processing 16 of 2096 batches (2631 sequences)\n",
      "Processing 17 of 2096 batches (2631 sequences)\n",
      "Processing 18 of 2096 batches (2564 sequences)\n",
      "Processing 19 of 2096 batches (2500 sequences)\n",
      "Processing 20 of 2096 batches (2500 sequences)\n",
      "Processing 21 of 2096 batches (2439 sequences)\n",
      "Processing 22 of 2096 batches (2439 sequences)\n",
      "Processing 23 of 2096 batches (2380 sequences)\n",
      "Processing 24 of 2096 batches (2380 sequences)\n",
      "Processing 25 of 2096 batches (2325 sequences)\n",
      "Processing 26 of 2096 batches (2325 sequences)\n",
      "Processing 27 of 2096 batches (2272 sequences)\n",
      "Processing 28 of 2096 batches (2272 sequences)\n",
      "Processing 29 of 2096 batches (2222 sequences)\n",
      "Processing 30 of 2096 batches (2222 sequences)\n",
      "Processing 31 of 2096 batches (2173 sequences)\n",
      "Processing 32 of 2096 batches (2173 sequences)\n",
      "Processing 33 of 2096 batches (2150 sequences)\n",
      "Processing 34 of 2096 batches (2127 sequences)\n",
      "Processing 35 of 2096 batches (2127 sequences)\n",
      "Processing 36 of 2096 batches (2083 sequences)\n",
      "Processing 37 of 2096 batches (2083 sequences)\n",
      "Processing 38 of 2096 batches (2040 sequences)\n",
      "Processing 39 of 2096 batches (2040 sequences)\n",
      "Processing 40 of 2096 batches (2040 sequences)\n",
      "Processing 41 of 2096 batches (2000 sequences)\n",
      "Processing 42 of 2096 batches (2000 sequences)\n",
      "Processing 43 of 2096 batches (2000 sequences)\n",
      "Processing 44 of 2096 batches (1960 sequences)\n",
      "Processing 45 of 2096 batches (1960 sequences)\n",
      "Processing 46 of 2096 batches (1960 sequences)\n",
      "Processing 47 of 2096 batches (1923 sequences)\n",
      "Processing 48 of 2096 batches (1923 sequences)\n",
      "Processing 49 of 2096 batches (1923 sequences)\n",
      "Processing 50 of 2096 batches (1886 sequences)\n",
      "Processing 51 of 2096 batches (1886 sequences)\n",
      "Processing 52 of 2096 batches (1886 sequences)\n",
      "Processing 53 of 2096 batches (1886 sequences)\n",
      "Processing 54 of 2096 batches (1851 sequences)\n",
      "Processing 55 of 2096 batches (1851 sequences)\n",
      "Processing 56 of 2096 batches (1851 sequences)\n",
      "Processing 57 of 2096 batches (1851 sequences)\n",
      "Processing 58 of 2096 batches (1851 sequences)\n",
      "Processing 59 of 2096 batches (1818 sequences)\n",
      "Processing 60 of 2096 batches (1818 sequences)\n",
      "Processing 61 of 2096 batches (1818 sequences)\n",
      "Processing 62 of 2096 batches (1818 sequences)\n",
      "Processing 63 of 2096 batches (1785 sequences)\n",
      "Processing 64 of 2096 batches (1785 sequences)\n",
      "Processing 65 of 2096 batches (1785 sequences)\n",
      "Processing 66 of 2096 batches (1785 sequences)\n",
      "Processing 67 of 2096 batches (1785 sequences)\n",
      "Processing 68 of 2096 batches (1754 sequences)\n",
      "Processing 69 of 2096 batches (1754 sequences)\n",
      "Processing 70 of 2096 batches (1754 sequences)\n",
      "Processing 71 of 2096 batches (1754 sequences)\n",
      "Processing 72 of 2096 batches (1724 sequences)\n",
      "Processing 73 of 2096 batches (1724 sequences)\n",
      "Processing 74 of 2096 batches (1724 sequences)\n",
      "Processing 75 of 2096 batches (1724 sequences)\n",
      "Processing 76 of 2096 batches (1694 sequences)\n",
      "Processing 77 of 2096 batches (1694 sequences)\n",
      "Processing 78 of 2096 batches (1694 sequences)\n",
      "Processing 79 of 2096 batches (1666 sequences)\n",
      "Processing 80 of 2096 batches (1666 sequences)\n",
      "Processing 81 of 2096 batches (1666 sequences)\n",
      "Processing 82 of 2096 batches (1666 sequences)\n",
      "Processing 83 of 2096 batches (1639 sequences)\n",
      "Processing 84 of 2096 batches (1639 sequences)\n",
      "Processing 85 of 2096 batches (1639 sequences)\n",
      "Processing 86 of 2096 batches (1639 sequences)\n",
      "Processing 87 of 2096 batches (1639 sequences)\n",
      "Processing 88 of 2096 batches (1612 sequences)\n",
      "Processing 89 of 2096 batches (1612 sequences)\n",
      "Processing 90 of 2096 batches (1612 sequences)\n",
      "Processing 91 of 2096 batches (1612 sequences)\n",
      "Processing 92 of 2096 batches (1587 sequences)\n",
      "Processing 93 of 2096 batches (1587 sequences)\n",
      "Processing 94 of 2096 batches (1587 sequences)\n",
      "Processing 95 of 2096 batches (1587 sequences)\n",
      "Processing 96 of 2096 batches (1587 sequences)\n",
      "Processing 97 of 2096 batches (1562 sequences)\n",
      "Processing 98 of 2096 batches (1562 sequences)\n",
      "Processing 99 of 2096 batches (1562 sequences)\n",
      "Processing 100 of 2096 batches (1562 sequences)\n",
      "Processing 101 of 2096 batches (1562 sequences)\n",
      "Processing 102 of 2096 batches (1562 sequences)\n",
      "Processing 103 of 2096 batches (1538 sequences)\n",
      "Processing 104 of 2096 batches (1538 sequences)\n",
      "Processing 105 of 2096 batches (1538 sequences)\n",
      "Processing 106 of 2096 batches (1538 sequences)\n",
      "Processing 107 of 2096 batches (1538 sequences)\n",
      "Processing 108 of 2096 batches (1515 sequences)\n",
      "Processing 109 of 2096 batches (1515 sequences)\n",
      "Processing 110 of 2096 batches (1515 sequences)\n",
      "Processing 111 of 2096 batches (1515 sequences)\n",
      "Processing 112 of 2096 batches (1515 sequences)\n",
      "Processing 113 of 2096 batches (1515 sequences)\n",
      "Processing 114 of 2096 batches (1492 sequences)\n",
      "Processing 115 of 2096 batches (1492 sequences)\n",
      "Processing 116 of 2096 batches (1492 sequences)\n",
      "Processing 117 of 2096 batches (1492 sequences)\n",
      "Processing 118 of 2096 batches (1492 sequences)\n",
      "Processing 119 of 2096 batches (1492 sequences)\n",
      "Processing 120 of 2096 batches (1470 sequences)\n",
      "Processing 121 of 2096 batches (1470 sequences)\n",
      "Processing 122 of 2096 batches (1470 sequences)\n",
      "Processing 123 of 2096 batches (1470 sequences)\n",
      "Processing 124 of 2096 batches (1470 sequences)\n",
      "Processing 125 of 2096 batches (1470 sequences)\n",
      "Processing 126 of 2096 batches (1470 sequences)\n",
      "Processing 127 of 2096 batches (1449 sequences)\n",
      "Processing 128 of 2096 batches (1449 sequences)\n",
      "Processing 129 of 2096 batches (1449 sequences)\n",
      "Processing 130 of 2096 batches (1449 sequences)\n",
      "Processing 131 of 2096 batches (1449 sequences)\n"
     ]
    }
   ],
   "source": [
    "model = esm_model\n",
    "model.eval()\n",
    "\n",
    "toks_per_batch = 100000\n",
    "fasta_file = './all_datasets_fasta.fasta'\n",
    "output_dir = '../data/results'\n",
    "output_file = 'run1'\n",
    "\n",
    "model = model.cuda()\n",
    "print(\"Transferred model to GPU\")\n",
    "\n",
    "dataset = FastaBatchedDataset.from_file('./all_datasets_fasta.fasta')\n",
    "batches = dataset.get_batch_indices(toks_per_batch, extra_toks_per_seq=1)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, collate_fn=alphabet.get_batch_converter(), batch_sampler=batches\n",
    ")\n",
    "print(f\"Read {fasta_file} with {len(dataset)} sequences\")\n",
    "\n",
    "output_dir = pathlib.Path(output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "return_contacts = False\n",
    "\n",
    "assert all(-(model.num_layers + 1) <= i <= model.num_layers for i in [33]) #[33] is the last layer of the thing\n",
    "repr_layers = [(i + model.num_layers + 1) % (model.num_layers + 1) for i in [33]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "        print(\n",
    "            f\"Processing {batch_idx + 1} of {len(batches)} batches ({toks.size(0)} sequences)\"\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "        out = model(toks, repr_layers=repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "        logits = out[\"logits\"].to(device=\"cpu\")\n",
    "        representations = {\n",
    "            layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()\n",
    "        }\n",
    "        \n",
    "        if return_contacts:\n",
    "            contacts = out[\"contacts\"].to(device=\"cpu\")\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            output_file = output_dir / f\"{label}.pt\"\n",
    "            output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            # assert os.path.is_dir(args.output_dir), f\"Output directory {args.output_dir} does not exist\"\n",
    "            result = {\"label\": label}\n",
    "            truncate_len = min(200, len(strs[i])) #TODO: Make that better, should be protien_len\n",
    "            # Call clone on tensors to ensure tensors are not views into a larger representation\n",
    "            # See https://github.com/pytorch/pytorch/issues/1995\n",
    "            \n",
    "            if (1): # \"per_tok\" in args.include:\n",
    "                result[\"representations\"] = {\n",
    "                    layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                    for layer, t in representations.items()\n",
    "                }\n",
    "            # if \"mean\" in args.include:\n",
    "            #     result[\"mean_representations\"] = {\n",
    "            #         layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "            #         for layer, t in representations.items()\n",
    "            #     }\n",
    "            # if \"bos\" in args.include:\n",
    "            #     result[\"bos_representations\"] = {\n",
    "            #         layer: t[i, 0].clone() for layer, t in representations.items()\n",
    "            #     }\n",
    "            # if return_contacts:\n",
    "            #     result[\"contacts\"] = contacts[i, : truncate_len, : truncate_len].clone()\n",
    "            # print(f\"Saving {args.output_file}\")\n",
    "            torch.save(\n",
    "                result,\n",
    "                output_file,\n",
    "            )\n",
    "            assert os.path.isfile(output_file), f\"Output file {output_file} does not exist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in toy_dataset.iterrows():\n",
    "    seq = row[1]['sequence']\n",
    "    print(row)\n",
    "    print(seq)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq_to_max_len(seq, max_len):\n",
    "    padding_token = '<pad>'\n",
    "    seq = seq + padding_token * (max_len - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pad sequences to all be <= a defined length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(row[1]['sequence_name'], pad_seq_to_max_len(row[1].sequence, protein_len)) for row in toy_dataset.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6952"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY AREA TO IMPROVE:\n",
    "\n",
    "Here we save the embeddings in a directory ran one-by-one... This is extremely inefficient given we have 1 million sequences... \\\n",
    "\n",
    "The below script will extract embeddings given a Fasta file efficiently.\n",
    "\n",
    "python esm/scripts/extract.py esm2_t33_650M_UR50D examples/data/some_proteins.fasta \\\n",
    "    examples/data/some_proteins_emb_esm2 --repr_layers 33 --include per_tok\n",
    "\n",
    "You can write Fasta files from pandas dataframes easily. Here's an example: https://bootstrappers.umassmed.edu/guides/main/r_writeFasta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6952it [04:44, 24.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, protein in tqdm.tqdm(enumerate(data)):\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([protein])\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "        results = esm_model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "    embeddings = results['representations'][33].detach().cpu().numpy()\n",
    "    sequence_name = protein[0]\n",
    "    # Modify all slashes in sequence name to dashes or sees name as directory, perhaps this is a good thing...\n",
    "    sequence_name = sequence_name.replace('/', '-')\n",
    "    torch.save(embeddings, f'../data/toy_esm_embeddings/{sequence_name}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_name = row[1]['sequence_name']\n",
    "sequence_name = sequence_name.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S6CDL6_9ACTN-62-159'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = torch.load(f'../data/toy_esm_embeddings/{sequence_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 202, 1280)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMFnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, classes, device='cuda', max_len=200):\n",
    "        self.dataset = dataset\n",
    "        self.classes = classes\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        self.class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        self.idx_to_class = {i: classes[i] for i in range(len(classes))}\n",
    "        self.letter_to_num = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n",
    "                'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n",
    "                'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19, \n",
    "                'N': 2, 'Y': 18, 'M': 12}\n",
    "        self.num_to_letter = {v:k for k, v in self.letter_to_num.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        sequence_name = row['sequence_name']\n",
    "        sequence_name = sequence_name.replace('/', '-')\n",
    "        embeddings = torch.load(f'../data/toy_esm_embeddings/{sequence_name}.pt')\n",
    "        embeddings = torch.tensor(embeddings, device='cuda', dtype=torch.float32)\n",
    "\n",
    "        '''Pad embeddings to max_len with zero vector'''\n",
    "        if embeddings.size(1) < self.max_len:\n",
    "            B, N, h = embeddings.size()\n",
    "            pad = torch.zeros((B, self.max_len - embeddings.shape[1], h), device=self.device)\n",
    "            embeddings = torch.cat((embeddings, pad), dim=1)\n",
    "\n",
    "        class_idx = torch.tensor(self.class_to_idx[row['family_accession']])\n",
    "        label = F.one_hot(class_idx, num_classes=len(self.classes))\n",
    "        return embeddings, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ESMFnDataset(toy_dataset, classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esmdl = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0243,  0.0271, -0.0837,  ..., -0.2799,  0.1995,  0.0577],\n",
      "         [-0.0919, -0.0847, -0.0205,  ..., -0.0418,  0.1840, -0.1400],\n",
      "         [ 0.1741,  0.1002,  0.0078,  ...,  0.0821,  0.0727, -0.0905],\n",
      "         ...,\n",
      "         [-0.0306, -0.0665,  0.0843,  ..., -0.0120, -0.0695, -0.1188],\n",
      "         [-0.0623,  0.0503,  0.0476,  ...,  0.0348, -0.0654, -0.0851],\n",
      "         [-0.0286,  0.0321,  0.0917,  ..., -0.0120, -0.0180, -0.0668]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch in esmdl:\n",
    "    print(batch[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1280, nheads=8, num_layers=6, device='cuda', classes=2, max_len=202, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.input_size, nhead=nheads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 32)\n",
    "        self.fc3 = torch.nn.Linear(32 * max_len, self.classes)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        if len(embedding.size()) > 3 and embedding.size(0) == 1:\n",
    "            embedding = embedding.squeeze(0)\n",
    "            assert len(embedding.size()) == 3, 'Embedding has greater than 4 dimensions'\n",
    "            \n",
    "        B, N, h = embedding.shape\n",
    "        hidden = self.transformer_encoder(embedding)\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        '''Flatten hidden state'''\n",
    "        hidden = hidden.view(B, -1)\n",
    "        hidden = self.fc3(hidden)\n",
    "        hidden = self.softmax(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(input_size=1280, nheads=8, num_layers=6, device=device, classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in esmdl:\n",
    "    embedding, label = batch\n",
    "    pred = model(embedding)\n",
    "    print(pred.shape) # shape is [batch_size, number of classes]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble running Learning to Split right now because of environment issues.\n",
    "Regardless you can focus on getting ESMFnDataset to work will all sequences.\n",
    "\n",
    "Honestly, I don't know if it's possible to run both this Transformer model on 1 million sequences nor Learning to Split but it seems like if there's anyone who can figure it out, it's you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mOSError: [Errno 122] Disk quota exceeded: '/afs/csail.mit.edu/u/j/johnyang/.local/share/jupyter/runtime/kernel-v2-14552M3O3Iqk1LseI.json'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mOSError: [Errno 122] Disk quota exceeded: '/afs/csail.mit.edu/u/j/johnyang/.local/share/jupyter/runtime/kernel-v2-14552M3O3Iqk1LseI.json'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# python scripts/extract.py esm2_t33_650M_UR50D examples/data/some_proteins.fasta \\\n",
    "#   examples/data/some_proteins_emb_esm2 --repr_layers 33 --include per_tok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7fa501b4aaeffe0e214aa0cbdcb67705c1047ca67f5d6d4c2be274f1571cd764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
