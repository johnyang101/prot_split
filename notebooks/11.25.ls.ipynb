{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Module imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"../google_prot_fns/\"\n",
    "filters = 64\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "protein_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = pd.read_csv('../data/2_class.csv')\n",
    "classes = pd.unique(toy_dataset['family_accession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get 10 random proteins from each class\n",
    "'''\n",
    "minimal_dataset = toy_dataset.groupby('family_accession').apply(lambda x: x.sample(10))\n",
    "minimal_dataset = minimal_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes) #Should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.gpu_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_gpu = get_free_gpu()\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esm_model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\") #Downloads cache to AFS. Limited space on AFS...\n",
    "# batch_converter = alphabet.get_batch_converter()\n",
    "# esm_model.to(device)\n",
    "# esm_model.eval()\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMFnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, classes, device='cuda', max_len=200):\n",
    "        self.dataset = dataset\n",
    "        self.classes = classes\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        self.class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        self.idx_to_class = {i: classes[i] for i in range(len(classes))}\n",
    "        self.letter_to_num = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n",
    "                'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n",
    "                'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19, \n",
    "                'N': 2, 'Y': 18, 'M': 12}\n",
    "        self.num_to_letter = {v:k for k, v in self.letter_to_num.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        sequence_name = row['sequence_name']\n",
    "        sequence_name = sequence_name.replace('/', '-')\n",
    "        embeddings = torch.load(f'../data/toy_esm_embeddings/{sequence_name}.pt')\n",
    "        embeddings = torch.tensor(embeddings, device='cuda', dtype=torch.float32)\n",
    "\n",
    "        '''Pad embeddings to max_len with zero vector'''\n",
    "        if embeddings.size(1) < self.max_len:\n",
    "            B, N, h = embeddings.size()\n",
    "            pad = torch.zeros((B, self.max_len - embeddings.shape[1], h), device=self.device)\n",
    "            embeddings = torch.cat((embeddings, pad), dim=1)\n",
    "\n",
    "        class_idx = torch.tensor(self.class_to_idx[row['family_accession']])\n",
    "        # label = F.one_hot(class_idx, num_classes=len(self.classes))\n",
    "        embeddings = embeddings.squeeze(0) #NOTE: LS adds its own batch dimension, so we need to remove it here\n",
    "        return embeddings, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ESMFnDataset(minimal_dataset, classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "esmdl = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0265,  0.0639,  0.0099,  ..., -0.2129,  0.2000, -0.0129],\n",
      "        [ 0.1280, -0.0193, -0.0608,  ...,  0.2596, -0.1939, -0.1739],\n",
      "        [ 0.1197, -0.0697,  0.0128,  ...,  0.0020,  0.1907, -0.2165],\n",
      "        ...,\n",
      "        [ 0.0513, -0.0610,  0.1865,  ...,  0.0707,  0.0377, -0.0421],\n",
      "        [ 0.0319, -0.0693,  0.1435,  ...,  0.0333,  0.0819,  0.0220],\n",
      "        [ 0.1033, -0.0212,  0.1197,  ...,  0.0796,  0.0763, -0.0332]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch in esmdl:\n",
    "    print(batch[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python scripts/extract.py esm2_t33_650M_UR50D examples/data/some_proteins.fasta \\\n",
    "#   examples/data/some_proteins_emb_esm2 --repr_layers 33 --include per_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ls.models.build import ModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ModelFactory.register(\"esm_transformer\")\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1280, nheads=8, num_layers=6, device='cuda', classes=2, max_len=202, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.input_size, nhead=nheads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 32)\n",
    "        self.fc3 = torch.nn.Linear(32 * max_len, self.classes)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        if len(embedding.size()) > 3 and embedding.size(0) == 1:\n",
    "            embedding = embedding.squeeze(0)\n",
    "            assert len(embedding.size()) == 3, 'Embedding has greater than 4 dimensions'\n",
    "            \n",
    "        B, N, h = embedding.shape\n",
    "        hidden = self.transformer_encoder(embedding)\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        '''Flatten hidden state'''\n",
    "        hidden = hidden.view(B, -1)\n",
    "        hidden = self.fc3(hidden)\n",
    "        hidden = self.softmax(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model esm_mlp already exists. Will replace it\n"
     ]
    }
   ],
   "source": [
    "@ModelFactory.register(\"esm_mlp\")\n",
    "class LinearLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1280, device='cuda', classes=2, max_len=202, **kwargs):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, 32)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(32 * max_len, self.classes)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, embedding):\n",
    "        if len(embedding.size()) > 3 and embedding.size(0) == 1:\n",
    "            embedding = embedding.squeeze(0)\n",
    "            assert len(embedding.size()) == 3, 'Embedding has greater than 4 dimensions'\n",
    "            \n",
    "        B, N, h = embedding.shape\n",
    "        # hidden = self.fc1(hidden)\n",
    "        # hidden = self.fc2(hidden)\n",
    "        '''Flatten hidden state'''\n",
    "        hidden = self.fc1(embedding)\n",
    "        hidden = self.relu1(hidden)\n",
    "        hidden = hidden.view(B, -1)\n",
    "        hidden = self.fc2(hidden)\n",
    "        # hidden = self.softmax(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(input_size=1280, nheads=8, num_layers=6, device=device, classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in esmdl:\n",
    "    embedding, label = batch\n",
    "    pred = model(embedding)\n",
    "    print(pred.shape) # shape is [batch_size, number of classes]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #000000; text-decoration-color: #000000\">12</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #000000; text-decoration-color: #000000\">03</span><span style=\"color: #000000; text-decoration-color: #000000\"> 17:30:25</span> </pre>\n"
      ],
      "text/plain": [
       "\u001b[39m2022\u001b[0m\u001b[39m-\u001b[0m\u001b[39m12\u001b[0m\u001b[39m-\u001b[0m\u001b[39m03\u001b[0m\u001b[39m \u001b[0m\u001b[39m17:30:25\u001b[0m "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">split_data\r</pre>\n"
      ],
      "text/plain": [
       "split_data\r"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m ls\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mTox21()\n\u001b[1;32m      3\u001b[0m \u001b[39m# Learning to split the Tox21 dataset.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Here we use a simple mlp as our model backbone and use roc_auc as the evaluation metric.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_data, test_data, train_indices, test_indices, splitter \u001b[39m=\u001b[39m ls\u001b[39m.\u001b[39mlearning_to_split(data, model\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmlp\u001b[39m\u001b[39m'\u001b[39m}, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m'\u001b[39m, return_order\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain_data\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_data\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain_indices\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_indices\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msplitter\u001b[39m\u001b[39m'\u001b[39m], num_outer_loop\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/ls/learning_to_split.py:63\u001b[0m, in \u001b[0;36mlearning_to_split\u001b[0;34m(data, return_order, verbose, **overwrite_config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m outer_loop \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cfg[\u001b[39m'\u001b[39m\u001b[39mnum_outer_loop\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m     \u001b[39m#### STEP 1 ####\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# Split the dataset using the Splitter\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# We start with random split for the first iteration\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     random_split \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m outer_loop \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     split_stats, train_indices, test_indices \u001b[39m=\u001b[39m split_data(\n\u001b[1;32m     64\u001b[0m         data, splitter, cfg, random_split)\n\u001b[1;32m     66\u001b[0m     \u001b[39m#### STEP 2 ####\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39m# Train the Predictor (from scratch) on the train split\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39m# (using a subset of train split for validation -- early stopping)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39m# Get the predictor.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     predictor \u001b[39m=\u001b[39m ModelFactory\u001b[39m.\u001b[39mget_model(cfg, predictor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/ls/training/split_data.py:33\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(data, splitter, cfg, random_split)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(progress_message, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, time\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m         \u001b[39m# Move the current batch onto the device\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(cfg[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(cfg[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = ls.datasets.Tox21()\n",
    "\n",
    "# Learning to split the Tox21 dataset.\n",
    "# Here we use a simple mlp as our model backbone and use roc_auc as the evaluation metric.\n",
    "train_data, test_data, train_indices, test_indices, splitter = ls.learning_to_split(data, model={'name': 'mlp'}, metric='roc_auc', return_order=['train_data', 'test_data', 'train_indices', 'test_indices', 'splitter'], num_outer_loop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202, 1280])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #000000; text-decoration-color: #000000\">12</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #000000; text-decoration-color: #000000\">03</span><span style=\"color: #000000; text-decoration-color: #000000\"> 17:29:33</span> </pre>\n"
      ],
      "text/plain": [
       "\u001b[39m2022\u001b[0m\u001b[39m-\u001b[0m\u001b[39m12\u001b[0m\u001b[39m-\u001b[0m\u001b[39m03\u001b[0m\u001b[39m \u001b[0m\u001b[39m17:29:33\u001b[0m "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">split_data\r</pre>\n"
      ],
      "text/plain": [
       "split_data\r"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                              \r</pre>\n"
      ],
      "text/plain": [
       "                              \r"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data, test_data, train_indices, test_indices, splitter \u001b[39m=\u001b[39m ls\u001b[39m.\u001b[39mlearning_to_split(dataset, model\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mesm_mlp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m: {}}, \n\u001b[1;32m      2\u001b[0m                                                                                     metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m'\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                                                                     return_order\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain_data\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_data\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain_indices\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_indices\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msplitter\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                                                                     batch_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_outer_loop\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/ls/learning_to_split.py:74\u001b[0m, in \u001b[0;36mlearning_to_split\u001b[0;34m(data, return_order, verbose, **overwrite_config)\u001b[0m\n\u001b[1;32m     71\u001b[0m predictor \u001b[39m=\u001b[39m ModelFactory\u001b[39m.\u001b[39mget_model(cfg, predictor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# Train it on the training split\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m val_score \u001b[39m=\u001b[39m train_predictor(data\u001b[39m=\u001b[39;49mdata, train_indices\u001b[39m=\u001b[39;49mtrain_indices,\n\u001b[1;32m     75\u001b[0m                             predictor\u001b[39m=\u001b[39;49mpredictor, cfg\u001b[39m=\u001b[39;49mcfg)\n\u001b[1;32m     76\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# Evaluate the train (validation) / test gap and print the split stats.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m test_score \u001b[39m=\u001b[39m test_predictor(data\u001b[39m=\u001b[39mdata, test_indices\u001b[39m=\u001b[39mtest_indices,\n\u001b[1;32m     79\u001b[0m                             predictor\u001b[39m=\u001b[39mpredictor, cfg\u001b[39m=\u001b[39mcfg)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/ls/training/train_predictor.py:57\u001b[0m, in \u001b[0;36mtrain_predictor\u001b[0;34m(data, train_indices, predictor, cfg)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[39m# train the predictor\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     predictor\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     59\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(cfg[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     60\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(cfg[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[0;32mIn [10], line 22\u001b[0m, in \u001b[0;36mESMFnDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m sequence_name \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39msequence_name\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m sequence_name \u001b[39m=\u001b[39m sequence_name\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/toy_esm_embeddings/\u001b[39;49m\u001b[39m{\u001b[39;49;00msequence_name\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(embeddings, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     25\u001b[0m \u001b[39m'''Pad embeddings to max_len with zero vector'''\u001b[39;00m\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/rsg/chemistry/johnyang/miniconda3/envs/ls/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_indices, test_indices, splitter = ls.learning_to_split(dataset, model={'name': 'esm_mlp', 'args': {}}, \n",
    "                                                                                    metric='roc_auc', num_workers=0,\n",
    "                                                                                    return_order=['train_data', 'test_data', 'train_indices', 'test_indices', 'splitter'],\n",
    "                                                                                    batch_size=20, patience=1, num_outer_loop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ls')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faa6ec85b2c0dea77078b7f7ca40a55a382c56ca263b3344d7c521ee91fc44c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
